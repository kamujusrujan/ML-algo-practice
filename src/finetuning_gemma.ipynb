{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e348cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a95615",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_PATH = '/run/media/srujan/Data/Development/ML-algo-practice/.cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3e691b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8743b52ed74446c9b584d8fb76f4b2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=quantization_config,cache_dir = CACHE_PATH\n",
    ").eval()\n",
    "\n",
    "\n",
    "# model = Gemma3ForCausalLM.from_pretrained(\n",
    "#     model_id,cache_dir = CACHE_PATH,\n",
    "# ).eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True,cache_dir = CACHE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01e2c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# messages = [\n",
    "#     [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "#         },{\n",
    "#             \"role\": \"assistant\",\n",
    "#             \"content\": [{\"type\": \"text\", \"text\": \"I dont know\"},]\n",
    "#         },{\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [{\"type\": \"text\", \"text\": \"Why not? give me a reason on why you cant do a poem\"},]\n",
    "#         },\n",
    "#     ],\n",
    "# ]\n",
    "\n",
    "\n",
    "# inputs = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt=True,\n",
    "#     tokenize=True,\n",
    "#     return_dict=True,\n",
    "#     return_tensors=\"pt\",\n",
    "# ).to(model.device).to(torch.bfloat16)\n",
    "\n",
    "\n",
    "# with torch.inference_mode():\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "# outputs = tokenizer.batch_decode(outputs)\n",
    "# print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fc82547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'translation', 'translation_extra'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# ds = load_dataset(\"nvidia/Nemotron-Post-Training-Dataset-v2\",  split=[\"code\", \"math\"] , cache_dir=CACHE_PATH)\n",
    "# data = load_dataset(\"Abirate/english_quotes\",cache_dir=CACHE_PATH)\n",
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\",cache_dir=CACHE_PATH)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5618be43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'The cup cracked and spilled its contents.',\n",
       " 'translation': 'Cracked and spilled its contents, the cup did.',\n",
       " 'translation_extra': 'Cracked and spilled its contents, the cup did. Hrmmm.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3903824b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear8bitLt(in_features=1152, out_features=1024, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=1152, out_features=256, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=1152, out_features=256, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=1024, out_features=1152, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=1152, out_features=6912, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=1152, out_features=6912, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=6912, out_features=1152, bias=False)\n",
       "          (act_fn): GELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0faa9b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def format_dataset(data_point) : \n",
    "\n",
    "    input_ = data_point['sentence']\n",
    "    output_ = data_point['translation']\n",
    "\n",
    "    # format the data into the required format for the SFT\n",
    "    MESSAGE_FORMAT = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a yoda translator.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": input_  },]\n",
    "        },{\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": output_},]\n",
    "        }\n",
    "    ]]\n",
    "\n",
    "    # return {\"messages\" : MESSAGE_FORMAT}\n",
    "\n",
    "    # encoded = tokenizer.apply_chat_template(\n",
    "    #     messages, \n",
    "    #     tokenize=True, \n",
    "    #     add_generation_prompt=False,\n",
    "    #     return_dict=True,\n",
    "        \n",
    "    #     # Truncation and padding might be needed depending on your max_seq_length\n",
    "    #     truncation=True,\n",
    "    #     max_length=512 \n",
    "    # )\n",
    "    \n",
    "    # # SFTTrainer expects input_ids and attention_mask\n",
    "    # return {\n",
    "    #     \"input_ids\": encoded[\"input_ids\"],\n",
    "    #     \"attention_mask\": encoded[\"attention_mask\"],\n",
    "    #     \"labels\": encoded[\"input_ids\"] # Language modeling tasks usually copy inputs to labels\n",
    "    # }\n",
    "\n",
    "    text = tokenizer.apply_chat_template(MESSAGE_FORMAT, tokenize=False)\n",
    "    return {\"text\" :text[0]} \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa68633c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<bos><start_of_turn>user\\nYou are a yoda translator.\\n\\nKick the ball straight and follow through.<end_of_turn>\\n<start_of_turn>model\\nStraight, kick the ball, and follow through.<end_of_turn>\\n'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_dataset(dataset[13])\n",
    "# dataset[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c6090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(format_dataset,remove_columns=dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2a1c525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<bos><start_of_turn>user\\nYou are a yoda translator.\\n\\nThe birch canoe slid on the smooth planks.<end_of_turn>\\n<start_of_turn>model\\nOn the smooth planks, the birch canoe slid.<end_of_turn>\\n'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c8159a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{ bos_token }}\\n{%- if messages[0][\\'role\\'] == \\'system\\' -%}\\n    {%- if messages[0][\\'content\\'] is string -%}\\n        {%- set first_user_prefix = messages[0][\\'content\\'] + \\'\\n\\n\\' -%}\\n    {%- else -%}\\n        {%- set first_user_prefix = messages[0][\\'content\\'][0][\\'text\\'] + \\'\\n\\n\\' -%}\\n    {%- endif -%}\\n    {%- set loop_messages = messages[1:] -%}\\n{%- else -%}\\n    {%- set first_user_prefix = \"\" -%}\\n    {%- set loop_messages = messages -%}\\n{%- endif -%}\\n{%- for message in loop_messages -%}\\n    {%- if (message[\\'role\\'] == \\'user\\') != (loop.index0 % 2 == 0) -%}\\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\\n    {%- endif -%}\\n    {%- if (message[\\'role\\'] == \\'assistant\\') -%}\\n        {%- set role = \"model\" -%}\\n    {%- else -%}\\n        {%- set role = message[\\'role\\'] -%}\\n    {%- endif -%}\\n    {{ \\'<start_of_turn>\\' + role + \\'\\n\\' + (first_user_prefix if loop.first else \"\") }}\\n    {%- if message[\\'content\\'] is string -%}\\n        {{ message[\\'content\\'] | trim }}\\n    {%- elif message[\\'content\\'] is iterable -%}\\n        {%- for item in message[\\'content\\'] -%}\\n            {%- if item[\\'type\\'] == \\'image\\' -%}\\n                {{ \\'<start_of_image>\\' }}\\n            {%- elif item[\\'type\\'] == \\'text\\' -%}\\n                {{ item[\\'text\\'] | trim }}\\n            {%- endif -%}\\n        {%- endfor -%}\\n    {%- else -%}\\n        {{ raise_exception(\"Invalid content type\") }}\\n    {%- endif -%}\\n    {{ \\'<end_of_turn>\\n\\' }}\\n{%- endfor -%}\\n{%- if add_generation_prompt -%}\\n    {{\\'<start_of_turn>model\\n\\'}}\\n{%- endif -%}\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "tokenizer.chat_template\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11f98cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.bfloat16. This is not supported.\n",
      "/run/media/srujan/Data/Development/ML-algo-practice/.venv/lib/python3.13/site-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos><start_of_turn>user\\nYou are a yoda translator.\\n\\nThe birch canoe slid on the smooth planks.<end_of_turn>\\n<start_of_turn>model\\nHmm, yes. Smooth planks, indeed. \\n\\nLet us see... \\n\\n*“The birch canoe slid... a gentle dance…”* \\n\\nIs that understood? <end_of_turn>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a yoda translator.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"The birch canoe slid on the smooth planks.\"  },]\n",
    "        }\n",
    "    ]]\n",
    "\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device).to(torch.bfloat16)\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "outputs = tokenizer.batch_decode(outputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cb2be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "611cecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=1000,\n",
    "        learning_rate=2e-4,\n",
    "        bf16=True,\n",
    "        logging_steps=10,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        dataloader_num_workers=2,             # <--- ADDED: Uses 2 CPU threads to load data\n",
    "        # dataset_kwargs={\"skip_prepare_dataset\": True} \n",
    "    ),\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=lora_config,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'pad_token_id': 3}.\n",
      "/run/media/srujan/Data/Development/ML-algo-practice/.venv/lib/python3.13/site-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  63/1000 03:20 < 51:16, 0.30 it/s, Epoch 0.69/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.203657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.588155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.779380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.384521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.220133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.085632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "outputs = tokenizer.batch_decode(outputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bfee29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad4d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ab145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
